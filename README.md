# A Literature Review on the BERT, RoBERTa and T5 NLP models

Investigated the differences between BERT and RoBERTa models and the T5 model.<br/>
The consequent impact of this difference was assessed.

BERT: Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018).<br/>
https://arxiv.org/abs/1810.04805

RoBERTa: Liu, Yinhan, et al. "Roberta: A robustly optimized bert pretraining approach." arXiv preprint arXiv:1907.11692 (2019).<br/>
https://arxiv.org/abs/1907.11692

T5: Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." J. Mach. Learn. Res. 21.140 (2020): 1-67.<br/>
https://arxiv.org/abs/1910.10683

Created in collaboration with:<br/>
Amulya Gupta Vangapalli
